{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from IPython.display import Image\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://www.pagina12.com.ar/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#para generar una solicitud usamos el modulo requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagina = BeautifulSoup(requests.get(URL).text,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsed_home(soup):\n",
    "  \n",
    "    s = soup.find_all('div',attrs={'class':'article-title'})\n",
    "        \n",
    "    links=[]\n",
    "        \n",
    "    for tag in s:\n",
    "        link = tag.a['href']\n",
    "        links.append(link)\n",
    "    return links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "listado_links = parsed_home(pagina)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error HTTPSConnectionPool(host='www.pagina13.com.ar', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f8d0c5afb20>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n"
     ]
    }
   ],
   "source": [
    "#Manejo de errores \n",
    "errores = []\n",
    "url_mala = URL.replace('2','3')\n",
    "try:\n",
    "    link = requests.get(url_mala)\n",
    "except Exception as e:\n",
    "    print('error', e)\n",
    "    errores.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.pagina12.com.ar/316084-el-gobierno-anuncia-al-mediodia-los-detalles-de-las-nuevas-m'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_nota = listado_links[0]\n",
    "url_nota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: pasar todo esto a una funcion!\n",
    "try:\n",
    "    url_p = requests.get(url_nota)\n",
    "    if url_p.status_code == 200:\n",
    "        url_bs = BeautifulSoup(url_p.text,'lxml')\n",
    "        #print(url_bs.prettify())\n",
    "        contenido = url_bs.find('div',attrs={'class':'article'})\n",
    "        \n",
    "        #guardar todo lo extraido en el diccionario! \n",
    "        ret_dic = {}\n",
    "    \n",
    "        #2 posibles formas de extraer la fecha de la pagina    \n",
    "        \n",
    "        fecha = contenido.find('div',attrs={'class':'time'}).span.text\n",
    "        #o se puede extraer la fecha como\n",
    "        # .get('datetime') para guardar en formato yyyy-mm-dd\n",
    "        fecha01 = contenido.find('span',attrs={'pubdate':'pubdate'}).get('datetime')\n",
    "        if fecha01:\n",
    "            ret_dic['fecha'] = fecha01\n",
    "        else:\n",
    "            ret_dic['fecha'] = None\n",
    "        \n",
    "        \n",
    "        titulo = contenido.h1.text\n",
    "        if titulo:\n",
    "            ret_dic['Titulo'] = titulo\n",
    "        else:\n",
    "            ret_dic['Titulo'] = None\n",
    "        \n",
    "        bajada = contenido.find('div',attrs={'class':'article-summary'}).text\n",
    "        if bajada:\n",
    "            ret_dic['bajada'] = bajada\n",
    "        else:\n",
    "            ret_dic['bajada'] = None\n",
    "        \n",
    "        \n",
    "        #extraccion del cuerpo\n",
    "        cuerpo = contenido.find_all('p',attrs={'class': None})\n",
    "        cuerpo_pag = []\n",
    "        for c in cuerpo:\n",
    "            cuerpo_pag.append(c.text)\n",
    "        \n",
    "        if cuerpo_pag:\n",
    "            ret_dic['cuerpo'] = cuerpo_pag\n",
    "        else:\n",
    "            ret_dic['cuerpo'] = None\n",
    "        \n",
    "        #extraccion de las imagenes\n",
    "        links_media = []\n",
    "        media = contenido.find('div',attrs={'class':'article-main-media-image'}).find_all('img')\n",
    "        len_media = len(media)\n",
    "        \n",
    "        if len_media == 0:\n",
    "            print('no hay imagenes')\n",
    "        else:\n",
    "            for link in media:\n",
    "                links_media.append(link['data-src'])\n",
    "        #validar si el link a elegir da una resp=uesta de 200\n",
    "        img_source = links_media[-1]\n",
    "        img_request = requests.get(img_source)\n",
    "        \n",
    "        if img_request.status_code == 200:\n",
    "            #imagen = Image(img_request.content)\n",
    "            ret_dic['imagen'] = img_source\n",
    "        \n",
    "        \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unificando el codigo para cuando se genere la nota\n",
    "def scrap_note(url):\n",
    "    try:\n",
    "        nota = requests.get(url)\n",
    "    except Exception as e:\n",
    "        print('Error scrapeando url', url)\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "    if nota.status_code == 200:\n",
    "        print(f'Error en obtener la pagina {url}')\n",
    "        print(f'Status code {nota.status_code}')\n",
    "        return None\n",
    "    \n",
    "    s_nota = BeautifulSoup(nota.text,'lxml')\n",
    "    ret_dic = #nombre de funcion obtenerInfo(s_nota)\n",
    "    ret_dic['url'] = url\n",
    "    return ret_dic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
